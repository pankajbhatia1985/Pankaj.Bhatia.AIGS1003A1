{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18de7949",
   "metadata": {},
   "source": [
    "# ASSIGNMENT#1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad698f2",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f810d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naiveBayes.py\n",
    "# -------------\n",
    "# Licensing Information: Please do not distribute or publish solutions to this\n",
    "# project. You are free to use and extend these projects for educational\n",
    "# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n",
    "# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n",
    "# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n",
    "\n",
    "import util\n",
    "import classificationMethod\n",
    "import math \n",
    "\n",
    "class NaiveBayesClassifier(classificationMethod.ClassificationMethod):\n",
    "    \"\"\"\n",
    "    See the project description for the specifications of the Naive Bayes classifier.\n",
    "    \n",
    "    Note that the variable 'datum' in this code refers to a counter of features\n",
    "    (not to a raw samples.Datum).\n",
    "    \"\"\"\n",
    "    def __init__(self, legalLabels):\n",
    "        self.legalLabels = legalLabels\n",
    "        self.type = \"naivebayes\"\n",
    "        self.k = 1 # this is the smoothing parameter, ** use it in your train method **\n",
    "        self.automaticTuning = False # Look at this flag to decide whether to choose k automatically ** use this in your train method **\n",
    "        \n",
    "    def setSmoothing(self, k):\n",
    "        \"\"\"\n",
    "        This is used by the main method to change the smoothing parameter before training.\n",
    "        Do not modify this method.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def train(self, trainingData, trainingLabels, validationData, validationLabels):\n",
    "        \"\"\"\n",
    "        Outside shell to call your method. Do not modify this method.\n",
    "        \"\"\"    \n",
    "            \n",
    "        # might be useful in your code later...\n",
    "        # this is a list of all features in the training set.\n",
    "        self.features = list(set([ f for datum in trainingData for f in datum.keys() ]));\n",
    "        \n",
    "        if (self.automaticTuning):\n",
    "                kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50]\n",
    "        else:\n",
    "                kgrid = [self.k]\n",
    "                \n",
    "        self.trainAndTune(trainingData, trainingLabels, validationData, validationLabels, kgrid)\n",
    "            \n",
    "    def trainAndTune(self, trainingData, trainingLabels, validationData, validationLabels, kgrid):\n",
    "        \"\"\"\n",
    "        Trains the classifier by collecting counts over the training data, and\n",
    "        stores the Laplace smoothed estimates so that they can be used to classify.\n",
    "        Evaluate each value of k in kgrid to choose the smoothing parameter \n",
    "        that gives the best accuracy on the held-out validationData.\n",
    "        \n",
    "        trainingData and validationData are lists of feature Counters.    The corresponding\n",
    "        label lists contain the correct label for each datum.\n",
    "        \n",
    "        To get the list of all possible features or labels, use self.features and \n",
    "        self.legalLabels.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        P = util.Counter()\n",
    "        for l in trainingLabels:\n",
    "            P[l] += 1\n",
    "        P.normalize()\n",
    "        self.P = P\n",
    "        \n",
    "        # Initialize stuff\n",
    "        counts = {}\n",
    "        totals = {}\n",
    "        for f in self.features:\n",
    "            counts[f] = {0: util.Counter(), 1: util.Counter()}\n",
    "            totals[f] = util.Counter()\n",
    "                     \n",
    "        # Calculate totals and counts\n",
    "        for i, datum in enumerate(trainingData):\n",
    "            y = trainingLabels[i]\n",
    "            for f, value in datum.items():\n",
    "                counts[f][value][y] += 1.0\n",
    "                totals[f][y] += 1.0 \n",
    "                \n",
    "        bestConditionals = {}\n",
    "        bestAccuracy = None\n",
    "        # Evaluate each k, and use the one that yields the best accuracy\n",
    "        for k in kgrid or [0.0]:\n",
    "            correct = 0\n",
    "            conditionals = {}            \n",
    "            for f in self.features:\n",
    "                conditionals[f] = {0: util.Counter(), 1: util.Counter()}\n",
    "                \n",
    "            # Run Laplace smoothing\n",
    "            for f in self.features:\n",
    "                for value in [0, 1]:\n",
    "                    for y in self.legalLabels:\n",
    "                        conditionals[f][value][y] = (counts[f][value][y] + k) / (totals[f][y] + k * 2)\n",
    "                \n",
    "            # Check the accuracy associated with this k\n",
    "            self.conditionals = conditionals              \n",
    "            guesses = self.classify(validationData)\n",
    "            for i, guess in enumerate(guesses):\n",
    "                correct += (validationLabels[i] == guess and 1.0 or 0.0)\n",
    "            accuracy = correct / len(guesses)\n",
    "            \n",
    "            # Keep the best k so far\n",
    "            if accuracy > bestAccuracy or bestAccuracy is None:\n",
    "                bestAccuracy = accuracy\n",
    "                bestConditionals = conditionals\n",
    "                self.k = k\n",
    "                \n",
    "        self.conditionals = bestConditionals\n",
    "                \n",
    "    def classify(self, testData):\n",
    "        \"\"\"\n",
    "        Classify the data based on the posterior distribution over labels.\n",
    "        \n",
    "        You shouldn't modify this method.\n",
    "        \"\"\"\n",
    "        guesses = []\n",
    "        self.posteriors = [] # Log posteriors are stored for later data analysis (autograder).\n",
    "        for datum in testData:\n",
    "            posterior = self.calculateLogJointProbabilities(datum)\n",
    "            guesses.append(posterior.argMax())\n",
    "            self.posteriors.append(posterior)\n",
    "        return guesses\n",
    "            \n",
    "    def calculateLogJointProbabilities(self, datum):\n",
    "        \"\"\"\n",
    "        Returns the log-joint distribution over legal labels and the datum.\n",
    "        Each log-probability should be stored in the log-joint counter, e.g.        \n",
    "        logJoint[3] = <Estimate of log( P(Label = 3, datum) )>\n",
    "        \n",
    "        To get the list of all possible features or labels, use self.features and \n",
    "        self.legalLabels.\n",
    "        \"\"\"\n",
    "        logJoint = util.Counter()\n",
    "        evidence = datum.items()\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        for y in self.legalLabels:\n",
    "            logJoint[y] = math.log(self.P[y])\n",
    "            for f in self.conditionals:\n",
    "                prob = self.conditionals[f][datum[f]][y]\n",
    "                logJoint[y] += (prob and math.log(prob) or 0.0)\n",
    "\n",
    "        return logJoint\n",
    "    \n",
    "    def findHighOddsFeatures(self, label1, label2):\n",
    "        \"\"\"\n",
    "        Returns the 100 best features for the odds ratio:\n",
    "                        P(feature = 1 | label1) / P(feature = 1 | label2) \n",
    "        \n",
    "        Note: you may find 'self.features' a useful way to loop through all possible features\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"        \n",
    "        featuresOdds = []\n",
    "        for f in self.features:\n",
    "            top = self.conditionals[f][1][label1]\n",
    "            bottom = self.conditionals[f][1][label2]\n",
    "            ratio = top / bottom\n",
    "            featuresOdds.append((f, ratio))\n",
    "            \n",
    "        featuresOdds = [f for f, odds in sorted(featuresOdds, key=lambda t: -t[1])[:100]]\n",
    "\n",
    "        return featuresOdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac6df8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing classification\n",
      "--------------------\n",
      "data:\t\tdigits\n",
      "classifier:\t\tnaiveBayes\n",
      "training set size:\t100\n",
      "using automatic tuning for naivebayes\n",
      "trying to read: digitdata/trainingimages\n",
      "testing\n",
      "trying to read: digitdata/traininglabels\n",
      "testing\n",
      "trying to read: digitdata/validationimages\n",
      "testing\n",
      "trying to read: digitdata/validationlabels\n",
      "testing\n",
      "trying to read: digitdata/testimages\n",
      "testing\n",
      "trying to read: digitdata/testlabels\n",
      "testing\n",
      "Extracting features...\n",
      "Training...\n",
      "Validating...\n",
      "('74', 'correct out of 100 (74.0%).')\n",
      "Testing...\n",
      "('65', 'correct out of 100 (65.0%).')\n",
      "===================================\n",
      "Mistake on example 3\n",
      "Predicted 3; truth is 5\n",
      "Image: \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "          +#########+       \n",
      "         +###########+      \n",
      "         ############+      \n",
      "         ############       \n",
      "         ####+++#####       \n",
      "         +##+     +++       \n",
      "         +###++++           \n",
      "          ########+         \n",
      "          #########+        \n",
      "          ##########+       \n",
      "          +##########       \n",
      "           +++  ++###+      \n",
      "                  +###      \n",
      "      ++           ###      \n",
      "     +###++       +###      \n",
      "      ######++   +###+      \n",
      "      ++#############       \n",
      "       ++###########+       \n",
      "         +#########+        \n",
      "           ++####++         \n",
      "                            \n",
      "                            \n",
      "                            \n"
     ]
    }
   ],
   "source": [
    "%run dataClassifier.py -c naiveBayes -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a6393a",
   "metadata": {},
   "source": [
    "### Link Referred: https://github.com/anthony-niklas/cs188/blob/master/p5/naiveBayes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e598ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
